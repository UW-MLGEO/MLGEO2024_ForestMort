{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95c0423-58be-4b7e-a1a5-1f8803b54853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zarr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38718781-f9d8-49b8-ae1c-98f8a4f5121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZarrDataset(Dataset):\n",
    "    '''\n",
    "    Torch dataset backed by a zarr array. It is assumed that\n",
    "    the first axis is the sample dimension.\n",
    "    '''\n",
    "    def __init__(self, store: str, minibatch_size: int=32, label_idx=12, **kwargs):\n",
    "        # Open the underlying array. This particular store is a 2-D array with shape (n_rows, n_columns).\n",
    "        self.array_ = zarr.open_array(store, mode=\"r\", **kwargs)\n",
    "        # Define how many samples we pull from the array at a time.\n",
    "        self.minibatch_size_ = minibatch_size\n",
    "        # Calculate length of the dataset. This \"length\" is the number of minibatches we can generate,\n",
    "        # NOT the number of samples we can generate.\n",
    "        self.len_ = int(np.ceil(self.array_.shape[0] / self.minibatch_size_))\n",
    "        # Track the index of the mortality column because I was messy setting it up lol\n",
    "        self.label_idx_ = label_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        # Simply return the length of the dataset\n",
    "        return self.len_\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate where in the array we will take a minibatch-sized sample.\n",
    "        start = idx * self.minibatch_size_\n",
    "        end = min(start + self.minibatch_size_, self.array_.shape[0])\n",
    "\n",
    "        # This line is actually pulling data over the internet\n",
    "        sel = self.array_[start:end, ...]\n",
    "\n",
    "        # These lines reshape the data and turn it into a tensor that torch can use\n",
    "        X = np.concatenate((sel[:, 0:self.label_idx_], sel[:, self.label_idx_+1:]), axis=1)\n",
    "        y = sel[:, self.label_idx_]\n",
    "\n",
    "        return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def collator(Xy):\n",
    "        # The output of __getitem__ is two arrays of shape (minibatch_size, n_columns) and \n",
    "        # (minibatch_size, 1). These are \"collated\" together to make new arrays of shape\n",
    "        # (minibatch_size * batch_size, n_columns) and (minibatch_size * batch_size, 1).\n",
    "        return (\n",
    "            torch.cat([sample[0] for sample in Xy], dim=0),\n",
    "            torch.cat([sample[1] for sample in Xy], dim=0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff220ae8-092d-4eca-b10c-4d341b5e0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = 128\n",
    "batch = 4\n",
    "train_store = \"gs://ads_training_data/single_pixel_year/training.zarr\"\n",
    "ds = ZarrDataset(train_store, minibatch_size=minibatch)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    ds, \n",
    "    batch_size=batch, \n",
    "    shuffle=True, \n",
    "    collate_fn=ZarrDataset.collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c2b11f-d6a4-4d06-b266-3bb76d091478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 372 ms, sys: 51.1 ms, total: 423 ms\n",
      "Wall time: 896 ms\n"
     ]
    }
   ],
   "source": [
    "# Time how long it takes to laod data and make sure\n",
    "# that the shape of the array is what we expect.\n",
    "%time X, y = next(iter(train_dataloader))\n",
    "assert X.shape[0] == minibatch * batch\n",
    "assert y.shape[0] == minibatch * batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b79873-8c5b-4f8e-a7fd-0c5de14980b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading time per epoch (sec): 15188.163\n"
     ]
    }
   ],
   "source": [
    "time_per_batch_ms = 779\n",
    "batches = np.ceil(ds.array_.shape[0] / (minibatch*batch))\n",
    "data_load_time = batches * time_per_batch_ms\n",
    "print(\"Data loading time per epoch (sec):\", data_load_time / 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
